{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "naCEqxNHr7ca",
    "outputId": "cd66a1e4-954d-4908-bf08-0c6a2b7cecc3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Flatten, Dense, Dropout, Lambda\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as K\n",
    "import torchvision\n",
    "from torchvision.transforms import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_m18EbaOsLDQ"
   },
   "outputs": [],
   "source": [
    "!unzip l2_cnn_liran.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zCSt9hZlsiia"
   },
   "outputs": [],
   "source": [
    "def labelMoreThenOne(data,label):\n",
    "    return len(list(filter(lambda item : item[1] == label ,data))) > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "duyQij0xtAOO"
   },
   "outputs": [],
   "source": [
    "def createImagesDataset(root_path):\n",
    "  size = 28\n",
    "  image_folder = torchvision.datasets.ImageFolder(root_path,transform=transforms.Compose([transforms.Resize((size,size)) ]))       \n",
    "  dataset = list(filter(lambda item :labelMoreThenOne(image_folder,item[1]),image_folder))  \n",
    "  num_examples = len(dataset)\n",
    "  data = np.zeros(shape=(num_examples,size,size))\n",
    "  labels = np.zeros(num_examples)\n",
    "  for index,(img , label) in enumerate(dataset):\n",
    "    arr = np.array(img)\n",
    "    data[index,:,:] = arr[:,:,0]\n",
    "    labels[index] = label\n",
    "\n",
    "  return data,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "colab_type": "code",
    "id": "t0J6NghZtCbO",
    "outputId": "2d42e61c-3f8e-4406-905c-759d1a8056af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  3.  3.  3.  3.  3.  3.\n",
      "  3.  3.  3.  3.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  5.  5.  5.  5.\n",
      "  5.  5.  5.  5.  5.  5.  6.  6.  6.  6.  6.  6.  6.  6.  6.  6.  7.  7.\n",
      "  7.  7.  7.  7.  7.  7.  7.  7.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.\n",
      "  9.  9.  9.  9.  9.  9.  9.  9.  9.  9. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10. 11. 11. 11. 11. 11. 11. 11. 11. 11. 11. 12. 12. 12. 12. 12. 12.\n",
      " 12. 12. 12. 12. 13. 13. 13. 13. 13. 13. 13. 13. 13. 13. 14. 14. 14. 14.\n",
      " 14. 14. 14. 14. 14. 14. 15. 15. 15. 15. 15. 15. 15. 15. 15. 15. 16. 16.\n",
      " 16. 16. 16. 16. 16. 16. 16. 16. 17. 17. 17. 17. 17. 17. 17. 17. 17. 17.\n",
      " 18. 18. 18. 18. 18. 18. 18. 18. 18. 18. 19. 19. 19. 19. 19. 19. 19. 19.\n",
      " 19. 19. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 21. 21. 21. 21. 21. 21.\n",
      " 21. 21. 21. 21. 22. 22. 22. 22. 22. 22. 22. 22. 22. 22. 23. 23. 23. 23.\n",
      " 23. 23. 23. 23. 23. 23. 24. 24. 24. 24. 24. 24. 24. 24. 24. 24. 25. 25.\n",
      " 25. 25. 25. 25. 25. 25. 25. 25. 26. 26. 26. 26. 26. 26. 26. 26. 26. 26.\n",
      " 27. 27. 27. 27. 27. 27. 27. 27. 27. 27. 28. 28. 28. 28. 28. 28. 28. 28.\n",
      " 28. 28. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 30. 30. 30. 30. 30. 30.\n",
      " 30. 30. 30. 30. 31. 31. 31. 31. 31. 31. 31. 31. 31. 31. 32. 32. 32. 32.\n",
      " 32. 32. 32. 32. 32. 32. 33. 33. 33. 33. 33. 33. 33. 33. 33. 33. 34. 34.\n",
      " 34. 34. 34. 34. 34. 34. 34. 34. 35. 35. 35. 35. 35. 35. 35. 35. 35. 35.\n",
      " 36. 36. 36. 36. 36. 36. 36. 36. 36. 36. 37. 37. 37. 37. 37. 37. 37. 37.\n",
      " 37. 37. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 39. 39. 39. 39. 39. 39.\n",
      " 39. 39. 39. 39.]\n"
     ]
    }
   ],
   "source": [
    "path_root_train = \"./train\"\n",
    "\n",
    "images,labels = createImagesDataset(path_root_train)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "X3cJCk0KIrIi",
    "outputId": "90231515-b628-4e50-899a-ec260147ece1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 28, 28)\n",
      "(400,)\n"
     ]
    }
   ],
   "source": [
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 10790
    },
    "colab_type": "code",
    "id": "ZkuSGRvNtJUu",
    "outputId": "1399bd3c-396f-42a8-b91e-188cbf05b4fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 28, 28)\n",
      "input_shape:  (28, 28)\n",
      "n  3\n",
      "images_indices  [array([ 12,  15,  48,  75, 129, 235]), array([ 11,  38,  76,  80, 104, 156]), array([ 13, 105, 107, 115, 136, 186]), array([ 52,  78, 135, 144, 201, 216]), array([ 33,  37,  79, 169, 195, 203, 206, 232]), array([ 20,  22,  42,  54, 126, 134, 159, 219]), array([ 69, 100, 193, 202, 214, 215, 229]), array([  3,  14,  55,  86, 120]), array([102, 146, 154, 190, 222]), array([ 23,  26,  53, 198, 217]), array([ 27,  93, 108, 122, 132, 194, 227]), array([  9,  94, 131, 141, 208]), array([ 19,  30,  56, 113, 130, 167, 236]), array([ 21,  44,  50, 103, 153, 175, 180, 223]), array([ 10,  25,  49,  96, 145, 211]), array([ 51,  60, 118, 119, 164, 183]), array([ 32,  83, 121, 138, 165, 171]), array([  0,  84,  87, 139, 221]), array([ 18, 106, 114, 124, 160, 197, 205, 234]), array([  4,   8,  66, 147, 176, 191]), array([ 24,  29,  72,  77, 157, 163, 177]), array([ 95, 117, 137, 150, 188]), array([ 40,  62, 123, 142, 178, 199, 239]), array([  5,  47,  85, 127, 149, 174, 218]), array([ 61,  65, 109, 110, 170, 224]), array([ 81,  90, 166, 168]), array([  2,  45,  57, 116, 226]), array([  1,  35,  41,  71, 112, 143, 162, 179]), array([ 82, 155, 185, 213, 233]), array([  7,  58,  63,  88, 125, 173]), array([ 16,  39,  73, 189, 237]), array([ 64,  98, 152, 182, 192]), array([ 17,  28, 151, 225]), array([ 34,  70,  89, 128, 140, 212, 230]), array([ 31,  43,  92, 101, 187, 210, 220]), array([ 67,  74,  91, 196, 209]), array([ 59,  99, 133, 181, 207]), array([  6,  46,  97, 111, 161, 231]), array([ 36, 148, 158, 184, 200, 238]), array([ 68, 172, 204, 228])]\n",
      "setClasses  40\n",
      "n  1\n",
      "images_indices  [array([ 60,  61, 111, 134]), array([19, 27, 72, 83]), array([116, 121, 143, 146]), array([33, 81, 82, 95]), array([ 80, 117]), array([16, 78]), array([ 44,  59, 135]), array([ 40,  99, 129, 136, 156]), array([  9,  10,  28, 142, 155]), array([ 6,  8, 25, 68, 69]), array([ 39, 109, 124]), array([ 38,  42,  46,  55, 151]), array([ 11, 147, 152]), array([ 98, 126]), array([ 18,  20,  26, 122]), array([  0,   4,  65, 133]), array([ 24, 106, 115, 159]), array([ 30,  31,  43,  93, 148]), array([73, 74]), array([ 17,  76, 127, 149]), array([ 12,  92, 102]), array([  3,  22, 125, 138, 154]), array([32, 70, 97]), array([53, 91, 94]), array([ 62,  86,  87, 113]), array([ 21,  48,  63,  85,  90, 100]), array([ 36,  57,  64, 118, 131]), array([  5, 123]), array([ 49,  56, 112, 120, 144]), array([ 88,  89,  96, 139]), array([ 66,  71, 103, 108, 158]), array([  2,  23,  67,  79, 119]), array([ 13,  14,  35,  41, 110, 141]), array([ 47, 114, 130]), array([101, 150, 153]), array([104, 105, 132, 137, 157]), array([ 45,  50,  51,  58, 145]), array([ 1, 34, 75, 77]), array([  7,  52,  54, 128]), array([ 15,  29,  37,  84, 107, 140])]\n",
      "setClasses  40\n",
      "Train on 240 samples, validate on 80 samples\n",
      "Epoch 1/300\n",
      "240/240 [==============================] - 0s 2ms/step - loss: 0.8539 - accuracy: 0.5017 - val_loss: 0.2592 - val_accuracy: 0.5000\n",
      "Epoch 2/300\n",
      "240/240 [==============================] - 0s 98us/step - loss: 0.2787 - accuracy: 0.4816 - val_loss: 0.2801 - val_accuracy: 0.5000\n",
      "Epoch 3/300\n",
      "240/240 [==============================] - 0s 88us/step - loss: 0.2619 - accuracy: 0.5446 - val_loss: 0.2933 - val_accuracy: 0.5000\n",
      "Epoch 4/300\n",
      "240/240 [==============================] - 0s 87us/step - loss: 0.2427 - accuracy: 0.5815 - val_loss: 0.2795 - val_accuracy: 0.5000\n",
      "Epoch 5/300\n",
      "240/240 [==============================] - 0s 89us/step - loss: 0.2267 - accuracy: 0.6479 - val_loss: 0.2564 - val_accuracy: 0.5000\n",
      "Epoch 6/300\n",
      "240/240 [==============================] - 0s 92us/step - loss: 0.2160 - accuracy: 0.6942 - val_loss: 0.2336 - val_accuracy: 0.5500\n",
      "Epoch 7/300\n",
      "240/240 [==============================] - 0s 82us/step - loss: 0.2252 - accuracy: 0.6027 - val_loss: 0.1972 - val_accuracy: 0.6750\n",
      "Epoch 8/300\n",
      "240/240 [==============================] - 0s 125us/step - loss: 0.2488 - accuracy: 0.5586 - val_loss: 0.2101 - val_accuracy: 0.6125\n",
      "Epoch 9/300\n",
      "240/240 [==============================] - 0s 96us/step - loss: 0.2252 - accuracy: 0.6378 - val_loss: 0.2091 - val_accuracy: 0.6250\n",
      "Epoch 10/300\n",
      "240/240 [==============================] - 0s 119us/step - loss: 0.2174 - accuracy: 0.6490 - val_loss: 0.2322 - val_accuracy: 0.5500\n",
      "Epoch 11/300\n",
      "240/240 [==============================] - 0s 90us/step - loss: 0.2114 - accuracy: 0.6925 - val_loss: 0.1917 - val_accuracy: 0.6750\n",
      "Epoch 12/300\n",
      "240/240 [==============================] - 0s 86us/step - loss: 0.2081 - accuracy: 0.6775 - val_loss: 0.1685 - val_accuracy: 0.7375\n",
      "Epoch 13/300\n",
      "240/240 [==============================] - 0s 89us/step - loss: 0.2017 - accuracy: 0.6769 - val_loss: 0.2281 - val_accuracy: 0.5625\n",
      "Epoch 14/300\n",
      "240/240 [==============================] - 0s 89us/step - loss: 0.2289 - accuracy: 0.6200 - val_loss: 0.2358 - val_accuracy: 0.5875\n",
      "Epoch 15/300\n",
      "240/240 [==============================] - 0s 96us/step - loss: 0.1955 - accuracy: 0.6842 - val_loss: 0.1830 - val_accuracy: 0.6750\n",
      "Epoch 16/300\n",
      "240/240 [==============================] - 0s 90us/step - loss: 0.1802 - accuracy: 0.7439 - val_loss: 0.2186 - val_accuracy: 0.6250\n",
      "Epoch 17/300\n",
      "240/240 [==============================] - 0s 90us/step - loss: 0.1944 - accuracy: 0.6607 - val_loss: 0.2088 - val_accuracy: 0.6625\n",
      "Epoch 18/300\n",
      "240/240 [==============================] - 0s 85us/step - loss: 0.1926 - accuracy: 0.7210 - val_loss: 0.1831 - val_accuracy: 0.6875\n",
      "Epoch 19/300\n",
      "240/240 [==============================] - 0s 86us/step - loss: 0.2016 - accuracy: 0.6864 - val_loss: 0.2664 - val_accuracy: 0.5250\n",
      "Epoch 20/300\n",
      "240/240 [==============================] - 0s 76us/step - loss: 0.2144 - accuracy: 0.6395 - val_loss: 0.2011 - val_accuracy: 0.6750\n",
      "Epoch 21/300\n",
      "240/240 [==============================] - 0s 88us/step - loss: 0.1882 - accuracy: 0.7266 - val_loss: 0.1619 - val_accuracy: 0.7625\n",
      "Epoch 22/300\n",
      "240/240 [==============================] - 0s 94us/step - loss: 0.1790 - accuracy: 0.7154 - val_loss: 0.2185 - val_accuracy: 0.6500\n",
      "Epoch 23/300\n",
      "240/240 [==============================] - 0s 81us/step - loss: 0.1777 - accuracy: 0.7662 - val_loss: 0.1648 - val_accuracy: 0.7375\n",
      "Epoch 24/300\n",
      "240/240 [==============================] - 0s 94us/step - loss: 0.1616 - accuracy: 0.7930 - val_loss: 0.1795 - val_accuracy: 0.7125\n",
      "Epoch 25/300\n",
      "240/240 [==============================] - 0s 100us/step - loss: 0.1657 - accuracy: 0.7483 - val_loss: 0.1869 - val_accuracy: 0.7125\n",
      "Epoch 26/300\n",
      "240/240 [==============================] - 0s 93us/step - loss: 0.1713 - accuracy: 0.7232 - val_loss: 0.1857 - val_accuracy: 0.7125\n",
      "Epoch 27/300\n",
      "240/240 [==============================] - 0s 87us/step - loss: 0.1541 - accuracy: 0.8047 - val_loss: 0.1608 - val_accuracy: 0.7625\n",
      "Epoch 28/300\n",
      "240/240 [==============================] - 0s 88us/step - loss: 0.1479 - accuracy: 0.7913 - val_loss: 0.1294 - val_accuracy: 0.8000\n",
      "Epoch 29/300\n",
      "240/240 [==============================] - 0s 81us/step - loss: 0.1538 - accuracy: 0.7796 - val_loss: 0.1574 - val_accuracy: 0.7625\n",
      "Epoch 30/300\n",
      "240/240 [==============================] - 0s 86us/step - loss: 0.1605 - accuracy: 0.7483 - val_loss: 0.1912 - val_accuracy: 0.7000\n",
      "Epoch 31/300\n",
      "240/240 [==============================] - 0s 88us/step - loss: 0.1647 - accuracy: 0.7757 - val_loss: 0.1474 - val_accuracy: 0.7875\n",
      "Epoch 32/300\n",
      "240/240 [==============================] - 0s 90us/step - loss: 0.1516 - accuracy: 0.7941 - val_loss: 0.1416 - val_accuracy: 0.7875\n",
      "Epoch 33/300\n",
      "240/240 [==============================] - 0s 93us/step - loss: 0.1342 - accuracy: 0.8460 - val_loss: 0.1455 - val_accuracy: 0.7750\n",
      "Epoch 34/300\n",
      "240/240 [==============================] - 0s 93us/step - loss: 0.1400 - accuracy: 0.8125 - val_loss: 0.1296 - val_accuracy: 0.8125\n",
      "Epoch 35/300\n",
      "240/240 [==============================] - 0s 80us/step - loss: 0.1318 - accuracy: 0.8605 - val_loss: 0.1311 - val_accuracy: 0.8125\n",
      "Epoch 36/300\n",
      "240/240 [==============================] - 0s 86us/step - loss: 0.1321 - accuracy: 0.8343 - val_loss: 0.1381 - val_accuracy: 0.8000\n",
      "Epoch 37/300\n",
      "240/240 [==============================] - 0s 86us/step - loss: 0.1280 - accuracy: 0.8371 - val_loss: 0.1171 - val_accuracy: 0.8500\n",
      "Epoch 38/300\n",
      "240/240 [==============================] - 0s 85us/step - loss: 0.1389 - accuracy: 0.8008 - val_loss: 0.1159 - val_accuracy: 0.8250\n",
      "Epoch 39/300\n",
      "240/240 [==============================] - 0s 87us/step - loss: 0.1592 - accuracy: 0.7227 - val_loss: 0.1277 - val_accuracy: 0.8375\n",
      "Epoch 40/300\n",
      "240/240 [==============================] - 0s 88us/step - loss: 0.1537 - accuracy: 0.7785 - val_loss: 0.1220 - val_accuracy: 0.8375\n",
      "Epoch 41/300\n",
      "240/240 [==============================] - 0s 102us/step - loss: 0.1194 - accuracy: 0.8544 - val_loss: 0.1271 - val_accuracy: 0.8125\n",
      "Epoch 42/300\n",
      "240/240 [==============================] - 0s 97us/step - loss: 0.1324 - accuracy: 0.8343 - val_loss: 0.1152 - val_accuracy: 0.8375\n",
      "Epoch 43/300\n",
      "240/240 [==============================] - 0s 92us/step - loss: 0.1317 - accuracy: 0.8108 - val_loss: 0.1399 - val_accuracy: 0.8250\n",
      "Epoch 44/300\n",
      "240/240 [==============================] - 0s 79us/step - loss: 0.1163 - accuracy: 0.8973 - val_loss: 0.1134 - val_accuracy: 0.8500\n",
      "Epoch 45/300\n",
      "240/240 [==============================] - 0s 79us/step - loss: 0.1295 - accuracy: 0.8287 - val_loss: 0.1196 - val_accuracy: 0.8500\n",
      "Epoch 46/300\n",
      "240/240 [==============================] - 0s 90us/step - loss: 0.1251 - accuracy: 0.8510 - val_loss: 0.1168 - val_accuracy: 0.8375\n",
      "Epoch 47/300\n",
      "240/240 [==============================] - 0s 89us/step - loss: 0.1334 - accuracy: 0.8253 - val_loss: 0.1235 - val_accuracy: 0.8250\n",
      "Epoch 48/300\n",
      "240/240 [==============================] - 0s 89us/step - loss: 0.1178 - accuracy: 0.8633 - val_loss: 0.1199 - val_accuracy: 0.8500\n",
      "Epoch 49/300\n",
      "240/240 [==============================] - 0s 97us/step - loss: 0.1056 - accuracy: 0.8839 - val_loss: 0.1243 - val_accuracy: 0.8375\n",
      "Epoch 50/300\n",
      "240/240 [==============================] - 0s 90us/step - loss: 0.1292 - accuracy: 0.8365 - val_loss: 0.1133 - val_accuracy: 0.8500\n",
      "Epoch 51/300\n",
      "240/240 [==============================] - 0s 85us/step - loss: 0.1381 - accuracy: 0.8170 - val_loss: 0.1137 - val_accuracy: 0.8500\n",
      "Epoch 52/300\n",
      "240/240 [==============================] - 0s 102us/step - loss: 0.1032 - accuracy: 0.8795 - val_loss: 0.1334 - val_accuracy: 0.8375\n",
      "Epoch 53/300\n",
      "240/240 [==============================] - 0s 82us/step - loss: 0.1080 - accuracy: 0.8828 - val_loss: 0.1181 - val_accuracy: 0.8375\n",
      "Epoch 54/300\n",
      "240/240 [==============================] - 0s 80us/step - loss: 0.1107 - accuracy: 0.8783 - val_loss: 0.1160 - val_accuracy: 0.8625\n",
      "Epoch 55/300\n",
      "240/240 [==============================] - 0s 85us/step - loss: 0.1008 - accuracy: 0.9163 - val_loss: 0.1349 - val_accuracy: 0.8000\n",
      "Epoch 56/300\n",
      "240/240 [==============================] - 0s 91us/step - loss: 0.1083 - accuracy: 0.8728 - val_loss: 0.1349 - val_accuracy: 0.8000\n",
      "Epoch 57/300\n",
      "240/240 [==============================] - 0s 88us/step - loss: 0.0996 - accuracy: 0.8940 - val_loss: 0.1283 - val_accuracy: 0.8125\n",
      "Epoch 58/300\n",
      "240/240 [==============================] - 0s 88us/step - loss: 0.1165 - accuracy: 0.8633 - val_loss: 0.1676 - val_accuracy: 0.7375\n",
      "Epoch 59/300\n",
      "240/240 [==============================] - 0s 80us/step - loss: 0.1237 - accuracy: 0.8599 - val_loss: 0.1202 - val_accuracy: 0.8125\n",
      "Epoch 60/300\n",
      "240/240 [==============================] - 0s 89us/step - loss: 0.1025 - accuracy: 0.8979 - val_loss: 0.1453 - val_accuracy: 0.8125\n",
      "Epoch 61/300\n",
      "240/240 [==============================] - 0s 91us/step - loss: 0.0948 - accuracy: 0.9129 - val_loss: 0.1110 - val_accuracy: 0.8625\n",
      "Epoch 62/300\n",
      "240/240 [==============================] - 0s 89us/step - loss: 0.1014 - accuracy: 0.8912 - val_loss: 0.1251 - val_accuracy: 0.8375\n",
      "Epoch 63/300\n",
      "240/240 [==============================] - 0s 88us/step - loss: 0.1175 - accuracy: 0.8320 - val_loss: 0.1188 - val_accuracy: 0.8250\n",
      "Epoch 64/300\n",
      "240/240 [==============================] - 0s 89us/step - loss: 0.1145 - accuracy: 0.8566 - val_loss: 0.1136 - val_accuracy: 0.8500\n",
      "Epoch 65/300\n",
      "240/240 [==============================] - 0s 97us/step - loss: 0.0867 - accuracy: 0.9247 - val_loss: 0.1078 - val_accuracy: 0.8625\n",
      "Epoch 66/300\n",
      "240/240 [==============================] - 0s 78us/step - loss: 0.1169 - accuracy: 0.8901 - val_loss: 0.1158 - val_accuracy: 0.8625\n",
      "Epoch 67/300\n",
      "240/240 [==============================] - 0s 107us/step - loss: 0.0892 - accuracy: 0.9247 - val_loss: 0.1673 - val_accuracy: 0.7875\n",
      "Epoch 68/300\n",
      "240/240 [==============================] - 0s 86us/step - loss: 0.1017 - accuracy: 0.8856 - val_loss: 0.1423 - val_accuracy: 0.8375\n",
      "Epoch 69/300\n",
      "240/240 [==============================] - 0s 104us/step - loss: 0.0936 - accuracy: 0.9118 - val_loss: 0.1116 - val_accuracy: 0.8750\n",
      "Epoch 70/300\n",
      "240/240 [==============================] - 0s 91us/step - loss: 0.0943 - accuracy: 0.9152 - val_loss: 0.1270 - val_accuracy: 0.8375\n",
      "Epoch 71/300\n",
      "240/240 [==============================] - 0s 91us/step - loss: 0.0932 - accuracy: 0.9068 - val_loss: 0.1526 - val_accuracy: 0.7750\n",
      "Epoch 72/300\n",
      "240/240 [==============================] - 0s 90us/step - loss: 0.1029 - accuracy: 0.8956 - val_loss: 0.1412 - val_accuracy: 0.8375\n",
      "Epoch 73/300\n",
      "240/240 [==============================] - 0s 87us/step - loss: 0.0881 - accuracy: 0.9135 - val_loss: 0.1110 - val_accuracy: 0.8750\n",
      "Epoch 74/300\n",
      "240/240 [==============================] - 0s 93us/step - loss: 0.0931 - accuracy: 0.9208 - val_loss: 0.1079 - val_accuracy: 0.8750\n",
      "Epoch 75/300\n",
      "240/240 [==============================] - 0s 96us/step - loss: 0.0796 - accuracy: 0.9408 - val_loss: 0.1096 - val_accuracy: 0.8375\n",
      "Epoch 76/300\n",
      "240/240 [==============================] - 0s 98us/step - loss: 0.0800 - accuracy: 0.9219 - val_loss: 0.1202 - val_accuracy: 0.8250\n",
      "Epoch 77/300\n",
      "240/240 [==============================] - 0s 81us/step - loss: 0.0824 - accuracy: 0.9269 - val_loss: 0.1275 - val_accuracy: 0.8125\n",
      "Epoch 78/300\n",
      "240/240 [==============================] - 0s 92us/step - loss: 0.0881 - accuracy: 0.9124 - val_loss: 0.1185 - val_accuracy: 0.8125\n",
      "Epoch 79/300\n",
      "240/240 [==============================] - 0s 88us/step - loss: 0.0804 - accuracy: 0.9375 - val_loss: 0.1132 - val_accuracy: 0.8625\n",
      "Epoch 80/300\n",
      "240/240 [==============================] - 0s 88us/step - loss: 0.0801 - accuracy: 0.9118 - val_loss: 0.1071 - val_accuracy: 0.8625\n",
      "Epoch 81/300\n",
      "240/240 [==============================] - 0s 87us/step - loss: 0.1064 - accuracy: 0.8873 - val_loss: 0.1156 - val_accuracy: 0.8375\n",
      "Epoch 82/300\n",
      "240/240 [==============================] - 0s 85us/step - loss: 0.0759 - accuracy: 0.9347 - val_loss: 0.1124 - val_accuracy: 0.8625\n",
      "Epoch 83/300\n",
      "240/240 [==============================] - 0s 86us/step - loss: 0.0751 - accuracy: 0.9448 - val_loss: 0.1401 - val_accuracy: 0.8000\n",
      "Epoch 84/300\n",
      "240/240 [==============================] - 0s 100us/step - loss: 0.0897 - accuracy: 0.8984 - val_loss: 0.1187 - val_accuracy: 0.8375\n",
      "Epoch 85/300\n",
      "240/240 [==============================] - 0s 95us/step - loss: 0.0800 - accuracy: 0.9297 - val_loss: 0.1215 - val_accuracy: 0.8500\n",
      "Epoch 86/300\n",
      "240/240 [==============================] - 0s 82us/step - loss: 0.0828 - accuracy: 0.9291 - val_loss: 0.1089 - val_accuracy: 0.8750\n",
      "Epoch 87/300\n",
      "240/240 [==============================] - 0s 86us/step - loss: 0.0880 - accuracy: 0.9291 - val_loss: 0.1193 - val_accuracy: 0.8125\n",
      "Epoch 88/300\n",
      "240/240 [==============================] - 0s 78us/step - loss: 0.0809 - accuracy: 0.9129 - val_loss: 0.1105 - val_accuracy: 0.8625\n",
      "Epoch 89/300\n",
      "240/240 [==============================] - 0s 86us/step - loss: 0.0794 - accuracy: 0.9509 - val_loss: 0.1048 - val_accuracy: 0.8750\n",
      "Epoch 90/300\n",
      "240/240 [==============================] - 0s 88us/step - loss: 0.0788 - accuracy: 0.9163 - val_loss: 0.1239 - val_accuracy: 0.8250\n",
      "Epoch 91/300\n",
      "240/240 [==============================] - 0s 95us/step - loss: 0.0885 - accuracy: 0.9208 - val_loss: 0.1123 - val_accuracy: 0.8875\n",
      "Epoch 92/300\n",
      "240/240 [==============================] - 0s 90us/step - loss: 0.0816 - accuracy: 0.9247 - val_loss: 0.1288 - val_accuracy: 0.8500\n",
      "Epoch 93/300\n",
      "240/240 [==============================] - 0s 90us/step - loss: 0.0900 - accuracy: 0.9369 - val_loss: 0.1385 - val_accuracy: 0.8250\n",
      "Epoch 94/300\n",
      "240/240 [==============================] - 0s 87us/step - loss: 0.0792 - accuracy: 0.9124 - val_loss: 0.1341 - val_accuracy: 0.8250\n",
      "Epoch 95/300\n",
      "240/240 [==============================] - 0s 87us/step - loss: 0.0674 - accuracy: 0.9520 - val_loss: 0.1196 - val_accuracy: 0.8375\n",
      "Epoch 96/300\n",
      "240/240 [==============================] - 0s 98us/step - loss: 0.0677 - accuracy: 0.9420 - val_loss: 0.1177 - val_accuracy: 0.8875\n",
      "Epoch 97/300\n",
      "240/240 [==============================] - 0s 94us/step - loss: 0.0784 - accuracy: 0.9152 - val_loss: 0.1203 - val_accuracy: 0.8500\n",
      "Epoch 98/300\n",
      "240/240 [==============================] - 0s 92us/step - loss: 0.0817 - accuracy: 0.9152 - val_loss: 0.1031 - val_accuracy: 0.8625\n",
      "Epoch 99/300\n",
      "240/240 [==============================] - 0s 85us/step - loss: 0.0648 - accuracy: 0.9760 - val_loss: 0.1257 - val_accuracy: 0.8375\n",
      "Epoch 100/300\n",
      "240/240 [==============================] - 0s 85us/step - loss: 0.0695 - accuracy: 0.9364 - val_loss: 0.1045 - val_accuracy: 0.9000\n",
      "Epoch 101/300\n",
      "240/240 [==============================] - 0s 93us/step - loss: 0.0702 - accuracy: 0.9503 - val_loss: 0.1137 - val_accuracy: 0.8625\n",
      "Epoch 102/300\n",
      "240/240 [==============================] - 0s 95us/step - loss: 0.0606 - accuracy: 0.9581 - val_loss: 0.1067 - val_accuracy: 0.8125\n",
      "Epoch 103/300\n",
      "240/240 [==============================] - 0s 88us/step - loss: 0.0683 - accuracy: 0.9431 - val_loss: 0.1049 - val_accuracy: 0.8625\n",
      "Epoch 104/300\n",
      "240/240 [==============================] - 0s 88us/step - loss: 0.0703 - accuracy: 0.9503 - val_loss: 0.1326 - val_accuracy: 0.8000\n",
      "Epoch 105/300\n",
      "240/240 [==============================] - 0s 89us/step - loss: 0.0740 - accuracy: 0.9431 - val_loss: 0.1041 - val_accuracy: 0.8750\n",
      "Epoch 106/300\n",
      "240/240 [==============================] - 0s 88us/step - loss: 0.0840 - accuracy: 0.9252 - val_loss: 0.1406 - val_accuracy: 0.8250\n",
      "Epoch 107/300\n",
      "240/240 [==============================] - 0s 90us/step - loss: 0.0817 - accuracy: 0.9308 - val_loss: 0.0991 - val_accuracy: 0.8625\n",
      "Epoch 108/300\n",
      "240/240 [==============================] - 0s 92us/step - loss: 0.0688 - accuracy: 0.9542 - val_loss: 0.1397 - val_accuracy: 0.8000\n",
      "Epoch 109/300\n",
      "240/240 [==============================] - 0s 86us/step - loss: 0.0717 - accuracy: 0.9275 - val_loss: 0.1080 - val_accuracy: 0.8625\n",
      "Epoch 110/300\n",
      "240/240 [==============================] - 0s 91us/step - loss: 0.0663 - accuracy: 0.9554 - val_loss: 0.0980 - val_accuracy: 0.8750\n",
      "Epoch 111/300\n",
      "240/240 [==============================] - 0s 94us/step - loss: 0.0733 - accuracy: 0.9157 - val_loss: 0.1169 - val_accuracy: 0.8250\n",
      "Epoch 112/300\n",
      "240/240 [==============================] - 0s 86us/step - loss: 0.0703 - accuracy: 0.9632 - val_loss: 0.1193 - val_accuracy: 0.8250\n",
      "Epoch 113/300\n",
      "240/240 [==============================] - 0s 92us/step - loss: 0.0584 - accuracy: 0.9654 - val_loss: 0.1055 - val_accuracy: 0.8875\n",
      "Epoch 114/300\n",
      "240/240 [==============================] - 0s 88us/step - loss: 0.0600 - accuracy: 0.9537 - val_loss: 0.1066 - val_accuracy: 0.8625\n",
      "Epoch 115/300\n",
      "240/240 [==============================] - 0s 78us/step - loss: 0.0775 - accuracy: 0.9353 - val_loss: 0.1126 - val_accuracy: 0.8500\n",
      "Epoch 116/300\n",
      "240/240 [==============================] - 0s 79us/step - loss: 0.0848 - accuracy: 0.9018 - val_loss: 0.1252 - val_accuracy: 0.8375\n",
      "Epoch 117/300\n",
      "240/240 [==============================] - 0s 95us/step - loss: 0.0585 - accuracy: 0.9654 - val_loss: 0.1150 - val_accuracy: 0.8375\n",
      "Epoch 118/300\n",
      "240/240 [==============================] - 0s 86us/step - loss: 0.0609 - accuracy: 0.9548 - val_loss: 0.1057 - val_accuracy: 0.8625\n",
      "Epoch 119/300\n",
      "240/240 [==============================] - 0s 86us/step - loss: 0.0530 - accuracy: 0.9710 - val_loss: 0.1514 - val_accuracy: 0.8125\n",
      "Epoch 120/300\n",
      "240/240 [==============================] - 0s 96us/step - loss: 0.0784 - accuracy: 0.9291 - val_loss: 0.1165 - val_accuracy: 0.8500\n",
      "Epoch 121/300\n",
      "240/240 [==============================] - 0s 85us/step - loss: 0.0756 - accuracy: 0.9090 - val_loss: 0.1127 - val_accuracy: 0.8875\n",
      "Epoch 122/300\n",
      "240/240 [==============================] - 0s 87us/step - loss: 0.0705 - accuracy: 0.9448 - val_loss: 0.1136 - val_accuracy: 0.8500\n",
      "Epoch 123/300\n",
      "240/240 [==============================] - 0s 93us/step - loss: 0.0624 - accuracy: 0.9559 - val_loss: 0.1242 - val_accuracy: 0.8500\n",
      "Epoch 124/300\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0559 - accuracy: 0.9621 - val_loss: 0.1177 - val_accuracy: 0.8500\n",
      "Epoch 125/300\n",
      "240/240 [==============================] - 0s 85us/step - loss: 0.0538 - accuracy: 0.9688 - val_loss: 0.0999 - val_accuracy: 0.8875\n",
      "Epoch 126/300\n",
      "240/240 [==============================] - 0s 89us/step - loss: 0.0578 - accuracy: 0.9676 - val_loss: 0.1156 - val_accuracy: 0.8500\n",
      "Epoch 127/300\n",
      "240/240 [==============================] - 0s 87us/step - loss: 0.0789 - accuracy: 0.9342 - val_loss: 0.1063 - val_accuracy: 0.8875\n",
      "Epoch 128/300\n",
      "240/240 [==============================] - 0s 101us/step - loss: 0.0717 - accuracy: 0.9291 - val_loss: 0.1029 - val_accuracy: 0.8625\n",
      "Epoch 129/300\n",
      "240/240 [==============================] - 0s 94us/step - loss: 0.0750 - accuracy: 0.9325 - val_loss: 0.1084 - val_accuracy: 0.8625\n",
      "Epoch 130/300\n",
      "240/240 [==============================] - 0s 92us/step - loss: 0.0769 - accuracy: 0.9347 - val_loss: 0.1123 - val_accuracy: 0.8750\n",
      "Epoch 131/300\n",
      "240/240 [==============================] - 0s 81us/step - loss: 0.0565 - accuracy: 0.9754 - val_loss: 0.1176 - val_accuracy: 0.8250\n",
      "Epoch 132/300\n",
      "240/240 [==============================] - 0s 99us/step - loss: 0.0552 - accuracy: 0.9609 - val_loss: 0.1069 - val_accuracy: 0.8750\n",
      "Epoch 133/300\n",
      "240/240 [==============================] - 0s 96us/step - loss: 0.0507 - accuracy: 0.9760 - val_loss: 0.1194 - val_accuracy: 0.8250\n",
      "Epoch 134/300\n",
      "240/240 [==============================] - 0s 90us/step - loss: 0.0566 - accuracy: 0.9754 - val_loss: 0.0964 - val_accuracy: 0.8625\n",
      "Epoch 135/300\n",
      "240/240 [==============================] - 0s 95us/step - loss: 0.0582 - accuracy: 0.9598 - val_loss: 0.1160 - val_accuracy: 0.8375\n",
      "Epoch 136/300\n",
      "240/240 [==============================] - 0s 83us/step - loss: 0.0542 - accuracy: 0.9665 - val_loss: 0.1233 - val_accuracy: 0.8625\n",
      "Epoch 137/300\n",
      "240/240 [==============================] - 0s 90us/step - loss: 0.0580 - accuracy: 0.9598 - val_loss: 0.1134 - val_accuracy: 0.8500\n",
      "Epoch 138/300\n",
      "240/240 [==============================] - 0s 92us/step - loss: 0.0631 - accuracy: 0.9632 - val_loss: 0.1221 - val_accuracy: 0.8375\n",
      "Epoch 139/300\n",
      "240/240 [==============================] - 0s 87us/step - loss: 0.0687 - accuracy: 0.9397 - val_loss: 0.1251 - val_accuracy: 0.8375\n",
      "Epoch 140/300\n",
      "240/240 [==============================] - 0s 94us/step - loss: 0.0771 - accuracy: 0.9269 - val_loss: 0.0999 - val_accuracy: 0.8875\n",
      "Epoch 141/300\n",
      "240/240 [==============================] - 0s 141us/step - loss: 0.0568 - accuracy: 0.9576 - val_loss: 0.1122 - val_accuracy: 0.8750\n",
      "Epoch 142/300\n",
      "240/240 [==============================] - 0s 90us/step - loss: 0.0622 - accuracy: 0.9453 - val_loss: 0.1106 - val_accuracy: 0.8625\n",
      "Epoch 143/300\n",
      "240/240 [==============================] - 0s 87us/step - loss: 0.0623 - accuracy: 0.9654 - val_loss: 0.1194 - val_accuracy: 0.8750\n",
      "Epoch 144/300\n",
      "240/240 [==============================] - 0s 90us/step - loss: 0.0522 - accuracy: 0.9609 - val_loss: 0.0988 - val_accuracy: 0.8625\n",
      "Epoch 145/300\n",
      "240/240 [==============================] - 0s 92us/step - loss: 0.0771 - accuracy: 0.9275 - val_loss: 0.1303 - val_accuracy: 0.8125\n",
      "Epoch 146/300\n",
      "240/240 [==============================] - 0s 93us/step - loss: 0.0579 - accuracy: 0.9632 - val_loss: 0.1088 - val_accuracy: 0.8625\n",
      "Epoch 147/300\n",
      "240/240 [==============================] - 0s 87us/step - loss: 0.0543 - accuracy: 0.9665 - val_loss: 0.1204 - val_accuracy: 0.8625\n",
      "Epoch 148/300\n",
      "240/240 [==============================] - 0s 89us/step - loss: 0.0460 - accuracy: 0.9872 - val_loss: 0.1117 - val_accuracy: 0.8500\n",
      "Epoch 149/300\n",
      "240/240 [==============================] - 0s 86us/step - loss: 0.0502 - accuracy: 0.9827 - val_loss: 0.1069 - val_accuracy: 0.8500\n",
      "Epoch 150/300\n",
      "240/240 [==============================] - 0s 86us/step - loss: 0.0521 - accuracy: 0.9587 - val_loss: 0.0960 - val_accuracy: 0.8875\n",
      "Epoch 151/300\n",
      "240/240 [==============================] - 0s 76us/step - loss: 0.0629 - accuracy: 0.9487 - val_loss: 0.1184 - val_accuracy: 0.8500\n",
      "Epoch 152/300\n",
      "240/240 [==============================] - 0s 89us/step - loss: 0.0703 - accuracy: 0.9448 - val_loss: 0.1069 - val_accuracy: 0.8500\n",
      "Epoch 153/300\n",
      "240/240 [==============================] - 0s 85us/step - loss: 0.0600 - accuracy: 0.9671 - val_loss: 0.0952 - val_accuracy: 0.8875\n",
      "Epoch 154/300\n",
      "240/240 [==============================] - 0s 93us/step - loss: 0.0598 - accuracy: 0.9464 - val_loss: 0.1087 - val_accuracy: 0.8500\n",
      "Epoch 155/300\n",
      "240/240 [==============================] - 0s 88us/step - loss: 0.0475 - accuracy: 0.9877 - val_loss: 0.1183 - val_accuracy: 0.8250\n",
      "Epoch 156/300\n",
      "240/240 [==============================] - 0s 92us/step - loss: 0.0514 - accuracy: 0.9743 - val_loss: 0.1006 - val_accuracy: 0.8750\n",
      "Epoch 157/300\n",
      "240/240 [==============================] - 0s 92us/step - loss: 0.0642 - accuracy: 0.9542 - val_loss: 0.1072 - val_accuracy: 0.8750\n",
      "Epoch 158/300\n",
      "240/240 [==============================] - 0s 98us/step - loss: 0.0605 - accuracy: 0.9526 - val_loss: 0.1003 - val_accuracy: 0.9000\n",
      "Epoch 159/300\n",
      "240/240 [==============================] - 0s 98us/step - loss: 0.0541 - accuracy: 0.9721 - val_loss: 0.1135 - val_accuracy: 0.8500\n",
      "Epoch 160/300\n",
      "240/240 [==============================] - 0s 81us/step - loss: 0.0473 - accuracy: 0.9632 - val_loss: 0.0928 - val_accuracy: 0.8875\n",
      "Epoch 161/300\n",
      "240/240 [==============================] - 0s 89us/step - loss: 0.0586 - accuracy: 0.9665 - val_loss: 0.1156 - val_accuracy: 0.8250\n",
      "Epoch 162/300\n",
      "240/240 [==============================] - 0s 88us/step - loss: 0.0455 - accuracy: 0.9916 - val_loss: 0.1102 - val_accuracy: 0.8750\n",
      "Epoch 163/300\n",
      "240/240 [==============================] - 0s 100us/step - loss: 0.0507 - accuracy: 0.9749 - val_loss: 0.1043 - val_accuracy: 0.8750\n",
      "Epoch 164/300\n",
      "240/240 [==============================] - 0s 92us/step - loss: 0.0409 - accuracy: 0.9877 - val_loss: 0.1027 - val_accuracy: 0.8500\n",
      "Epoch 165/300\n",
      "240/240 [==============================] - 0s 86us/step - loss: 0.0567 - accuracy: 0.9548 - val_loss: 0.1014 - val_accuracy: 0.8625\n",
      "Epoch 166/300\n",
      "240/240 [==============================] - 0s 88us/step - loss: 0.0683 - accuracy: 0.9492 - val_loss: 0.1014 - val_accuracy: 0.8750\n",
      "Epoch 167/300\n",
      "240/240 [==============================] - 0s 92us/step - loss: 0.0840 - accuracy: 0.9012 - val_loss: 0.1098 - val_accuracy: 0.8750\n",
      "Epoch 168/300\n",
      "240/240 [==============================] - 0s 94us/step - loss: 0.0581 - accuracy: 0.9581 - val_loss: 0.1154 - val_accuracy: 0.8000\n",
      "Epoch 169/300\n",
      "240/240 [==============================] - 0s 85us/step - loss: 0.0618 - accuracy: 0.9559 - val_loss: 0.1034 - val_accuracy: 0.8625\n",
      "Epoch 170/300\n",
      "240/240 [==============================] - 0s 87us/step - loss: 0.0485 - accuracy: 0.9671 - val_loss: 0.0963 - val_accuracy: 0.8750\n",
      "Epoch 171/300\n",
      "240/240 [==============================] - 0s 90us/step - loss: 0.0372 - accuracy: 0.9872 - val_loss: 0.1051 - val_accuracy: 0.8500\n",
      "Epoch 172/300\n",
      "240/240 [==============================] - 0s 96us/step - loss: 0.0416 - accuracy: 0.9743 - val_loss: 0.1014 - val_accuracy: 0.8625\n",
      "Epoch 173/300\n",
      "240/240 [==============================] - 0s 91us/step - loss: 0.0421 - accuracy: 0.9788 - val_loss: 0.0940 - val_accuracy: 0.9000\n",
      "Epoch 174/300\n",
      "240/240 [==============================] - 0s 87us/step - loss: 0.0607 - accuracy: 0.9570 - val_loss: 0.1126 - val_accuracy: 0.8500\n",
      "Epoch 175/300\n",
      "240/240 [==============================] - 0s 90us/step - loss: 0.0613 - accuracy: 0.9414 - val_loss: 0.1046 - val_accuracy: 0.9000\n",
      "Epoch 176/300\n",
      "240/240 [==============================] - 0s 91us/step - loss: 0.0569 - accuracy: 0.9593 - val_loss: 0.1023 - val_accuracy: 0.8500\n",
      "Epoch 177/300\n",
      "240/240 [==============================] - 0s 86us/step - loss: 0.0463 - accuracy: 0.9621 - val_loss: 0.1063 - val_accuracy: 0.8750\n",
      "Epoch 178/300\n",
      "240/240 [==============================] - 0s 88us/step - loss: 0.0421 - accuracy: 0.9844 - val_loss: 0.1070 - val_accuracy: 0.8375\n",
      "Epoch 179/300\n",
      "240/240 [==============================] - 0s 92us/step - loss: 0.0418 - accuracy: 0.9782 - val_loss: 0.1236 - val_accuracy: 0.8375\n",
      "Epoch 180/300\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0613 - accuracy: 0.9615 - val_loss: 0.1058 - val_accuracy: 0.8750\n",
      "Epoch 181/300\n",
      "240/240 [==============================] - 0s 94us/step - loss: 0.0392 - accuracy: 0.9883 - val_loss: 0.1042 - val_accuracy: 0.8500\n",
      "Epoch 182/300\n",
      "240/240 [==============================] - 0s 92us/step - loss: 0.0548 - accuracy: 0.9593 - val_loss: 0.1164 - val_accuracy: 0.8250\n",
      "Epoch 183/300\n",
      "240/240 [==============================] - 0s 102us/step - loss: 0.0465 - accuracy: 0.9587 - val_loss: 0.1253 - val_accuracy: 0.8250\n",
      "Epoch 184/300\n",
      "240/240 [==============================] - 0s 98us/step - loss: 0.0543 - accuracy: 0.9492 - val_loss: 0.1110 - val_accuracy: 0.8500\n",
      "Epoch 185/300\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0512 - accuracy: 0.9699 - val_loss: 0.1032 - val_accuracy: 0.8750\n",
      "Epoch 186/300\n",
      "240/240 [==============================] - 0s 90us/step - loss: 0.0764 - accuracy: 0.9085 - val_loss: 0.1018 - val_accuracy: 0.8625\n",
      "Epoch 187/300\n",
      "240/240 [==============================] - 0s 88us/step - loss: 0.0597 - accuracy: 0.9542 - val_loss: 0.1105 - val_accuracy: 0.8625\n",
      "Epoch 188/300\n",
      "240/240 [==============================] - 0s 86us/step - loss: 0.0563 - accuracy: 0.9626 - val_loss: 0.1072 - val_accuracy: 0.8500\n",
      "Epoch 189/300\n",
      "240/240 [==============================] - 0s 90us/step - loss: 0.0513 - accuracy: 0.9604 - val_loss: 0.1125 - val_accuracy: 0.8375\n",
      "Epoch 190/300\n",
      "240/240 [==============================] - 0s 79us/step - loss: 0.0661 - accuracy: 0.9503 - val_loss: 0.1079 - val_accuracy: 0.8750\n",
      "Epoch 191/300\n",
      "240/240 [==============================] - 0s 90us/step - loss: 0.0434 - accuracy: 0.9872 - val_loss: 0.1004 - val_accuracy: 0.8500\n",
      "Epoch 192/300\n",
      "240/240 [==============================] - 0s 88us/step - loss: 0.0400 - accuracy: 0.9883 - val_loss: 0.0924 - val_accuracy: 0.9125\n",
      "Epoch 193/300\n",
      "240/240 [==============================] - 0s 83us/step - loss: 0.0455 - accuracy: 0.9665 - val_loss: 0.1194 - val_accuracy: 0.8500\n",
      "Epoch 194/300\n",
      "240/240 [==============================] - 0s 79us/step - loss: 0.0474 - accuracy: 0.9754 - val_loss: 0.1140 - val_accuracy: 0.8625\n",
      "Epoch 195/300\n",
      "240/240 [==============================] - 0s 88us/step - loss: 0.0440 - accuracy: 0.9743 - val_loss: 0.0965 - val_accuracy: 0.9000\n",
      "Epoch 196/300\n",
      "240/240 [==============================] - 0s 93us/step - loss: 0.0414 - accuracy: 0.9799 - val_loss: 0.1017 - val_accuracy: 0.8625\n",
      "Epoch 197/300\n",
      "240/240 [==============================] - 0s 89us/step - loss: 0.0401 - accuracy: 0.9877 - val_loss: 0.1069 - val_accuracy: 0.8625\n",
      "Epoch 198/300\n",
      "240/240 [==============================] - 0s 108us/step - loss: 0.0558 - accuracy: 0.9621 - val_loss: 0.1073 - val_accuracy: 0.8625\n",
      "Epoch 199/300\n",
      "240/240 [==============================] - 0s 88us/step - loss: 0.0539 - accuracy: 0.9749 - val_loss: 0.0987 - val_accuracy: 0.8750\n",
      "Epoch 200/300\n",
      "240/240 [==============================] - 0s 93us/step - loss: 0.0535 - accuracy: 0.9715 - val_loss: 0.1041 - val_accuracy: 0.8750\n",
      "Epoch 201/300\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0499 - accuracy: 0.9621 - val_loss: 0.1081 - val_accuracy: 0.8625\n",
      "Epoch 202/300\n",
      "240/240 [==============================] - 0s 97us/step - loss: 0.0373 - accuracy: 0.9916 - val_loss: 0.1119 - val_accuracy: 0.8500\n",
      "Epoch 203/300\n",
      "240/240 [==============================] - 0s 94us/step - loss: 0.0436 - accuracy: 0.9704 - val_loss: 0.0964 - val_accuracy: 0.8750\n",
      "Epoch 204/300\n",
      "240/240 [==============================] - 0s 89us/step - loss: 0.0470 - accuracy: 0.9754 - val_loss: 0.1181 - val_accuracy: 0.8000\n",
      "Epoch 205/300\n",
      "240/240 [==============================] - 0s 95us/step - loss: 0.0378 - accuracy: 0.9872 - val_loss: 0.0948 - val_accuracy: 0.8750\n",
      "Epoch 206/300\n",
      "240/240 [==============================] - 0s 95us/step - loss: 0.0340 - accuracy: 0.9911 - val_loss: 0.1062 - val_accuracy: 0.8500\n",
      "Epoch 207/300\n",
      "240/240 [==============================] - 0s 88us/step - loss: 0.0436 - accuracy: 0.9794 - val_loss: 0.1036 - val_accuracy: 0.8625\n",
      "Epoch 208/300\n",
      "240/240 [==============================] - 0s 88us/step - loss: 0.0413 - accuracy: 0.9754 - val_loss: 0.0999 - val_accuracy: 0.8500\n",
      "Epoch 209/300\n",
      "240/240 [==============================] - 0s 80us/step - loss: 0.0550 - accuracy: 0.9464 - val_loss: 0.1008 - val_accuracy: 0.8875\n",
      "Epoch 210/300\n",
      "240/240 [==============================] - 0s 109us/step - loss: 0.0389 - accuracy: 0.9872 - val_loss: 0.1012 - val_accuracy: 0.8625\n",
      "Epoch 211/300\n",
      "240/240 [==============================] - 0s 96us/step - loss: 0.0545 - accuracy: 0.9788 - val_loss: 0.1159 - val_accuracy: 0.8500\n",
      "Epoch 212/300\n",
      "240/240 [==============================] - 0s 93us/step - loss: 0.0561 - accuracy: 0.9565 - val_loss: 0.0928 - val_accuracy: 0.9000\n",
      "Epoch 213/300\n",
      "240/240 [==============================] - 0s 88us/step - loss: 0.0402 - accuracy: 0.9660 - val_loss: 0.1022 - val_accuracy: 0.8750\n",
      "Epoch 214/300\n",
      "240/240 [==============================] - 0s 91us/step - loss: 0.0369 - accuracy: 0.9833 - val_loss: 0.0939 - val_accuracy: 0.8875\n",
      "Epoch 215/300\n",
      "240/240 [==============================] - 0s 90us/step - loss: 0.0392 - accuracy: 0.9922 - val_loss: 0.1112 - val_accuracy: 0.8500\n",
      "Epoch 216/300\n",
      "240/240 [==============================] - 0s 87us/step - loss: 0.0460 - accuracy: 0.9704 - val_loss: 0.0960 - val_accuracy: 0.8625\n",
      "Epoch 217/300\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0606 - accuracy: 0.9626 - val_loss: 0.1037 - val_accuracy: 0.8500\n",
      "Epoch 218/300\n",
      "240/240 [==============================] - 0s 96us/step - loss: 0.0445 - accuracy: 0.9883 - val_loss: 0.1028 - val_accuracy: 0.8500\n",
      "Epoch 219/300\n",
      "240/240 [==============================] - 0s 82us/step - loss: 0.0325 - accuracy: 0.9833 - val_loss: 0.0951 - val_accuracy: 0.8750\n",
      "Epoch 220/300\n",
      "240/240 [==============================] - 0s 89us/step - loss: 0.0394 - accuracy: 0.9782 - val_loss: 0.1062 - val_accuracy: 0.8750\n",
      "Epoch 221/300\n",
      "240/240 [==============================] - 0s 96us/step - loss: 0.0322 - accuracy: 0.9911 - val_loss: 0.0928 - val_accuracy: 0.8875\n",
      "Epoch 222/300\n",
      "240/240 [==============================] - 0s 92us/step - loss: 0.0391 - accuracy: 0.9754 - val_loss: 0.1001 - val_accuracy: 0.8500\n",
      "Epoch 223/300\n",
      "240/240 [==============================] - 0s 91us/step - loss: 0.0417 - accuracy: 0.9799 - val_loss: 0.1157 - val_accuracy: 0.8500\n",
      "Epoch 224/300\n",
      "240/240 [==============================] - 0s 86us/step - loss: 0.0526 - accuracy: 0.9665 - val_loss: 0.0997 - val_accuracy: 0.8750\n",
      "Epoch 225/300\n",
      "240/240 [==============================] - 0s 87us/step - loss: 0.0464 - accuracy: 0.9805 - val_loss: 0.1010 - val_accuracy: 0.8625\n",
      "Epoch 226/300\n",
      "240/240 [==============================] - 0s 87us/step - loss: 0.0415 - accuracy: 0.9872 - val_loss: 0.1107 - val_accuracy: 0.8625\n",
      "Epoch 227/300\n",
      "240/240 [==============================] - 0s 87us/step - loss: 0.0462 - accuracy: 0.9743 - val_loss: 0.1037 - val_accuracy: 0.8750\n",
      "Epoch 228/300\n",
      "240/240 [==============================] - 0s 103us/step - loss: 0.0519 - accuracy: 0.9827 - val_loss: 0.0932 - val_accuracy: 0.9000\n",
      "Epoch 229/300\n",
      "240/240 [==============================] - 0s 85us/step - loss: 0.0490 - accuracy: 0.9805 - val_loss: 0.1158 - val_accuracy: 0.8750\n",
      "Epoch 230/300\n",
      "240/240 [==============================] - 0s 99us/step - loss: 0.0345 - accuracy: 0.9794 - val_loss: 0.1111 - val_accuracy: 0.8750\n",
      "Epoch 231/300\n",
      "240/240 [==============================] - 0s 108us/step - loss: 0.0431 - accuracy: 0.9665 - val_loss: 0.0962 - val_accuracy: 0.8625\n",
      "Epoch 232/300\n",
      "240/240 [==============================] - 0s 90us/step - loss: 0.0356 - accuracy: 0.9916 - val_loss: 0.0981 - val_accuracy: 0.8750\n",
      "Epoch 233/300\n",
      "240/240 [==============================] - 0s 82us/step - loss: 0.0378 - accuracy: 0.9799 - val_loss: 0.0967 - val_accuracy: 0.8750\n",
      "Epoch 234/300\n",
      "240/240 [==============================] - 0s 89us/step - loss: 0.0289 - accuracy: 0.9922 - val_loss: 0.1042 - val_accuracy: 0.8750\n",
      "Epoch 235/300\n",
      "240/240 [==============================] - 0s 99us/step - loss: 0.0329 - accuracy: 0.9833 - val_loss: 0.0833 - val_accuracy: 0.9250\n",
      "Epoch 236/300\n",
      "240/240 [==============================] - 0s 105us/step - loss: 0.0371 - accuracy: 0.9833 - val_loss: 0.0914 - val_accuracy: 0.8750\n",
      "Epoch 237/300\n",
      "240/240 [==============================] - 0s 81us/step - loss: 0.0635 - accuracy: 0.9408 - val_loss: 0.0959 - val_accuracy: 0.8500\n",
      "Epoch 238/300\n",
      "240/240 [==============================] - 0s 81us/step - loss: 0.0516 - accuracy: 0.9682 - val_loss: 0.1033 - val_accuracy: 0.8625\n",
      "Epoch 239/300\n",
      "240/240 [==============================] - 0s 86us/step - loss: 0.0597 - accuracy: 0.9342 - val_loss: 0.1028 - val_accuracy: 0.8750\n",
      "Epoch 240/300\n",
      "240/240 [==============================] - 0s 92us/step - loss: 0.0392 - accuracy: 0.9782 - val_loss: 0.0926 - val_accuracy: 0.8750\n",
      "Epoch 241/300\n",
      "240/240 [==============================] - 0s 89us/step - loss: 0.0553 - accuracy: 0.9593 - val_loss: 0.1044 - val_accuracy: 0.8625\n",
      "Epoch 242/300\n",
      "240/240 [==============================] - 0s 90us/step - loss: 0.0404 - accuracy: 0.9704 - val_loss: 0.1006 - val_accuracy: 0.8875\n",
      "Epoch 243/300\n",
      "240/240 [==============================] - 0s 91us/step - loss: 0.0325 - accuracy: 0.9872 - val_loss: 0.1151 - val_accuracy: 0.8625\n",
      "Epoch 244/300\n",
      "240/240 [==============================] - 0s 97us/step - loss: 0.0467 - accuracy: 0.9715 - val_loss: 0.0970 - val_accuracy: 0.9125\n",
      "Epoch 245/300\n",
      "240/240 [==============================] - 0s 102us/step - loss: 0.0397 - accuracy: 0.9754 - val_loss: 0.1007 - val_accuracy: 0.8625\n",
      "Epoch 246/300\n",
      "240/240 [==============================] - 0s 85us/step - loss: 0.0364 - accuracy: 0.9805 - val_loss: 0.1079 - val_accuracy: 0.8875\n",
      "Epoch 247/300\n",
      "240/240 [==============================] - 0s 83us/step - loss: 0.0356 - accuracy: 0.9743 - val_loss: 0.0956 - val_accuracy: 0.8750\n",
      "Epoch 248/300\n",
      "240/240 [==============================] - 0s 89us/step - loss: 0.0355 - accuracy: 0.9788 - val_loss: 0.1018 - val_accuracy: 0.9000\n",
      "Epoch 249/300\n",
      "240/240 [==============================] - 0s 92us/step - loss: 0.0320 - accuracy: 0.9916 - val_loss: 0.0954 - val_accuracy: 0.9000\n",
      "Epoch 250/300\n",
      "240/240 [==============================] - 0s 94us/step - loss: 0.0450 - accuracy: 0.9721 - val_loss: 0.1095 - val_accuracy: 0.9000\n",
      "Epoch 251/300\n",
      "240/240 [==============================] - 0s 90us/step - loss: 0.0473 - accuracy: 0.9749 - val_loss: 0.0996 - val_accuracy: 0.8875\n",
      "Epoch 252/300\n",
      "240/240 [==============================] - 0s 91us/step - loss: 0.0348 - accuracy: 0.9961 - val_loss: 0.0884 - val_accuracy: 0.8875\n",
      "Epoch 253/300\n",
      "240/240 [==============================] - 0s 86us/step - loss: 0.0508 - accuracy: 0.9715 - val_loss: 0.0943 - val_accuracy: 0.8875\n",
      "Epoch 254/300\n",
      "240/240 [==============================] - 0s 82us/step - loss: 0.0340 - accuracy: 0.9916 - val_loss: 0.0936 - val_accuracy: 0.8875\n",
      "Epoch 255/300\n",
      "240/240 [==============================] - 0s 106us/step - loss: 0.0463 - accuracy: 0.9665 - val_loss: 0.0975 - val_accuracy: 0.8750\n",
      "Epoch 256/300\n",
      "240/240 [==============================] - 0s 80us/step - loss: 0.0438 - accuracy: 0.9626 - val_loss: 0.1206 - val_accuracy: 0.8250\n",
      "Epoch 257/300\n",
      "240/240 [==============================] - 0s 98us/step - loss: 0.0363 - accuracy: 0.9838 - val_loss: 0.0931 - val_accuracy: 0.8875\n",
      "Epoch 258/300\n",
      "240/240 [==============================] - 0s 95us/step - loss: 0.0271 - accuracy: 0.9916 - val_loss: 0.1003 - val_accuracy: 0.8625\n",
      "Epoch 259/300\n",
      "240/240 [==============================] - 0s 98us/step - loss: 0.0266 - accuracy: 0.9955 - val_loss: 0.1034 - val_accuracy: 0.8625\n",
      "Epoch 260/300\n",
      "240/240 [==============================] - 0s 91us/step - loss: 0.0470 - accuracy: 0.9570 - val_loss: 0.1090 - val_accuracy: 0.8875\n",
      "Epoch 261/300\n",
      "240/240 [==============================] - 0s 91us/step - loss: 0.0444 - accuracy: 0.9794 - val_loss: 0.1000 - val_accuracy: 0.8750\n",
      "Epoch 262/300\n",
      "240/240 [==============================] - 0s 87us/step - loss: 0.0466 - accuracy: 0.9593 - val_loss: 0.1168 - val_accuracy: 0.8250\n",
      "Epoch 263/300\n",
      "240/240 [==============================] - 0s 88us/step - loss: 0.0385 - accuracy: 0.9710 - val_loss: 0.1093 - val_accuracy: 0.8625\n",
      "Epoch 264/300\n",
      "240/240 [==============================] - 0s 92us/step - loss: 0.0371 - accuracy: 0.9833 - val_loss: 0.0944 - val_accuracy: 0.8625\n",
      "Epoch 265/300\n",
      "240/240 [==============================] - 0s 87us/step - loss: 0.0260 - accuracy: 1.0000 - val_loss: 0.0993 - val_accuracy: 0.8875\n",
      "Epoch 266/300\n",
      "240/240 [==============================] - 0s 86us/step - loss: 0.0281 - accuracy: 0.9955 - val_loss: 0.0961 - val_accuracy: 0.8750\n",
      "Epoch 267/300\n",
      "240/240 [==============================] - 0s 88us/step - loss: 0.0302 - accuracy: 0.9916 - val_loss: 0.0984 - val_accuracy: 0.8625\n",
      "Epoch 268/300\n",
      "240/240 [==============================] - 0s 93us/step - loss: 0.0322 - accuracy: 0.9866 - val_loss: 0.1057 - val_accuracy: 0.8500\n",
      "Epoch 269/300\n",
      "240/240 [==============================] - 0s 89us/step - loss: 0.0500 - accuracy: 0.9498 - val_loss: 0.1096 - val_accuracy: 0.8750\n",
      "Epoch 270/300\n",
      "240/240 [==============================] - 0s 90us/step - loss: 0.0480 - accuracy: 0.9743 - val_loss: 0.0979 - val_accuracy: 0.8750\n",
      "Epoch 271/300\n",
      "240/240 [==============================] - 0s 89us/step - loss: 0.0360 - accuracy: 0.9872 - val_loss: 0.0981 - val_accuracy: 0.8750\n",
      "Epoch 272/300\n",
      "240/240 [==============================] - 0s 111us/step - loss: 0.0376 - accuracy: 0.9844 - val_loss: 0.0970 - val_accuracy: 0.8750\n",
      "Epoch 273/300\n",
      "240/240 [==============================] - 0s 86us/step - loss: 0.0290 - accuracy: 0.9955 - val_loss: 0.1043 - val_accuracy: 0.8875\n",
      "Epoch 274/300\n",
      "240/240 [==============================] - 0s 93us/step - loss: 0.0314 - accuracy: 0.9844 - val_loss: 0.0920 - val_accuracy: 0.8625\n",
      "Epoch 275/300\n",
      "240/240 [==============================] - 0s 109us/step - loss: 0.0405 - accuracy: 0.9738 - val_loss: 0.1139 - val_accuracy: 0.8750\n",
      "Epoch 276/300\n",
      "240/240 [==============================] - 0s 92us/step - loss: 0.0496 - accuracy: 0.9676 - val_loss: 0.0928 - val_accuracy: 0.8625\n",
      "Epoch 277/300\n",
      "240/240 [==============================] - 0s 93us/step - loss: 0.0377 - accuracy: 0.9699 - val_loss: 0.0996 - val_accuracy: 0.8625\n",
      "Epoch 278/300\n",
      "240/240 [==============================] - 0s 86us/step - loss: 0.0564 - accuracy: 0.9425 - val_loss: 0.0927 - val_accuracy: 0.8750\n",
      "Epoch 279/300\n",
      "240/240 [==============================] - 0s 92us/step - loss: 0.0383 - accuracy: 0.9715 - val_loss: 0.0887 - val_accuracy: 0.8875\n",
      "Epoch 280/300\n",
      "240/240 [==============================] - 0s 100us/step - loss: 0.0356 - accuracy: 0.9838 - val_loss: 0.0988 - val_accuracy: 0.8625\n",
      "Epoch 281/300\n",
      "240/240 [==============================] - 0s 91us/step - loss: 0.0259 - accuracy: 0.9866 - val_loss: 0.0965 - val_accuracy: 0.8875\n",
      "Epoch 282/300\n",
      "240/240 [==============================] - 0s 90us/step - loss: 0.0262 - accuracy: 0.9911 - val_loss: 0.0910 - val_accuracy: 0.8875\n",
      "Epoch 283/300\n",
      "240/240 [==============================] - 0s 96us/step - loss: 0.0253 - accuracy: 0.9955 - val_loss: 0.0982 - val_accuracy: 0.8750\n",
      "Epoch 284/300\n",
      "240/240 [==============================] - 0s 83us/step - loss: 0.0371 - accuracy: 0.9704 - val_loss: 0.0974 - val_accuracy: 0.8750\n",
      "Epoch 285/300\n",
      "240/240 [==============================] - 0s 90us/step - loss: 0.0310 - accuracy: 0.9877 - val_loss: 0.0855 - val_accuracy: 0.9000\n",
      "Epoch 286/300\n",
      "240/240 [==============================] - 0s 91us/step - loss: 0.0318 - accuracy: 0.9794 - val_loss: 0.1152 - val_accuracy: 0.8625\n",
      "Epoch 287/300\n",
      "240/240 [==============================] - 0s 92us/step - loss: 0.0444 - accuracy: 0.9609 - val_loss: 0.0891 - val_accuracy: 0.9000\n",
      "Epoch 288/300\n",
      "240/240 [==============================] - 0s 92us/step - loss: 0.0448 - accuracy: 0.9760 - val_loss: 0.0953 - val_accuracy: 0.8500\n",
      "Epoch 289/300\n",
      "240/240 [==============================] - 0s 97us/step - loss: 0.0340 - accuracy: 0.9883 - val_loss: 0.0969 - val_accuracy: 0.8750\n",
      "Epoch 290/300\n",
      "240/240 [==============================] - 0s 85us/step - loss: 0.0284 - accuracy: 0.9833 - val_loss: 0.1125 - val_accuracy: 0.8375\n",
      "Epoch 291/300\n",
      "240/240 [==============================] - 0s 79us/step - loss: 0.0412 - accuracy: 0.9704 - val_loss: 0.1040 - val_accuracy: 0.8750\n",
      "Epoch 292/300\n",
      "240/240 [==============================] - 0s 85us/step - loss: 0.0372 - accuracy: 0.9788 - val_loss: 0.0954 - val_accuracy: 0.8625\n",
      "Epoch 293/300\n",
      "240/240 [==============================] - 0s 88us/step - loss: 0.0252 - accuracy: 0.9922 - val_loss: 0.0940 - val_accuracy: 0.8750\n",
      "Epoch 294/300\n",
      "240/240 [==============================] - 0s 78us/step - loss: 0.0295 - accuracy: 0.9782 - val_loss: 0.1084 - val_accuracy: 0.8625\n",
      "Epoch 295/300\n",
      "240/240 [==============================] - 0s 96us/step - loss: 0.0340 - accuracy: 0.9838 - val_loss: 0.0920 - val_accuracy: 0.8750\n",
      "Epoch 296/300\n",
      "240/240 [==============================] - 0s 102us/step - loss: 0.0335 - accuracy: 0.9760 - val_loss: 0.1041 - val_accuracy: 0.8750\n",
      "Epoch 297/300\n",
      "240/240 [==============================] - 0s 101us/step - loss: 0.0347 - accuracy: 0.9782 - val_loss: 0.0984 - val_accuracy: 0.8875\n",
      "Epoch 298/300\n",
      "240/240 [==============================] - 0s 95us/step - loss: 0.0259 - accuracy: 0.9827 - val_loss: 0.0972 - val_accuracy: 0.8750\n",
      "Epoch 299/300\n",
      "240/240 [==============================] - 0s 88us/step - loss: 0.0355 - accuracy: 0.9827 - val_loss: 0.1072 - val_accuracy: 0.8500\n",
      "Epoch 300/300\n",
      "240/240 [==============================] - 0s 87us/step - loss: 0.0351 - accuracy: 0.9827 - val_loss: 0.1017 - val_accuracy: 0.8750\n",
      "* Accuracy on training set: 99.58%\n",
      "* Accuracy on test set: 87.50%\n"
     ]
    }
   ],
   "source": [
    "epochs = 300\n",
    "\n",
    "\n",
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n",
    "    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n",
    "\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n",
    "\n",
    "\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    '''Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    '''\n",
    "    margin = 1\n",
    "    square_pred = K.square(y_pred)\n",
    "    margin_square = K.square(K.maximum(margin - y_pred, 0))\n",
    "    return K.mean(y_true * square_pred + (1 - y_true) * margin_square)\n",
    "  \n",
    "\n",
    "def create_pairs(x, images_indices,setClasses):\n",
    "    '''Positive and negative pair creation.\n",
    "    Alternates between positive and negative pairs.\n",
    "    '''\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    n = min([len(images_indices[index]) for index,d in enumerate(setClasses)]) - 1\n",
    "    print(\"n \", n)\n",
    "    print(\"images_indices \", images_indices)\n",
    "    print(\"setClasses \", len(setClasses))\n",
    "    for index,c in enumerate(setClasses):\n",
    "        for i in range(n):\n",
    "            z1, z2 = images_indices[index][i], images_indices[index][i + 1]\n",
    "            pairs += [[x[z1], x[z2]]]\n",
    "            inc = random.randrange(1, len(setClasses))\n",
    "            dn = (index + inc) % len(setClasses) - 1\n",
    "            z1, z2 = images_indices[index][i], images_indices[dn][i]\n",
    "            pairs += [[x[z1], x[z2]]]\n",
    "            labels += [1, 0]\n",
    "    return np.array(pairs), np.array(labels)\n",
    "\n",
    "\n",
    "def create_base_network(input_shape):\n",
    "    '''Base network to be shared (eq. to feature extraction).\n",
    "    '''\n",
    "    input = Input(shape=input_shape)\n",
    "    x = Flatten()(input)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    return Model(input, x)\n",
    "\n",
    "\n",
    "def compute_accuracy(y_true, y_pred):\n",
    "    '''Compute classification accuracy with a fixed threshold on distances.\n",
    "    '''\n",
    "    pred = y_pred.ravel() < 0.5\n",
    "    return np.mean(pred == y_true)\n",
    "\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    '''Compute classification accuracy with a fixed threshold on distances.\n",
    "    '''\n",
    "    return K.mean(K.equal(y_true, K.cast(y_pred < 0.5, y_true.dtype)))\n",
    "\n",
    "\n",
    "# the data, split between train and test sets\n",
    "print(images.shape)\n",
    "x_train, x_test, y_train, y_test  = train_test_split(images,labels,train_size =0.6,test_size=0.4)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "input_shape = x_train.shape[1:]\n",
    "print(\"input_shape: \", input_shape)\n",
    "\n",
    "# create training+test positive and negative pairs\n",
    "images_indices = [np.where(y_train == i)[0] for i in list(set(y_train))]\n",
    "tr_pairs, tr_y = create_pairs(x_train, images_indices,list(set(y_train)))\n",
    "\n",
    "images_indices = [np.where(y_test == i)[0] for i in list(set(y_test))]\n",
    "te_pairs, te_y = create_pairs(x_test, images_indices,list(set(y_test)))\n",
    "\n",
    "# network definition\n",
    "base_network = create_base_network(input_shape)\n",
    "\n",
    "input_a = Input(shape=input_shape)\n",
    "input_b = Input(shape=input_shape)\n",
    "\n",
    "# because we re-use the same instance `base_network`,\n",
    "# the weights of the network\n",
    "# will be shared across the two branches\n",
    "processed_a = base_network(input_a)\n",
    "processed_b = base_network(input_b)\n",
    "\n",
    "distance = Lambda(euclidean_distance,\n",
    "                  output_shape=eucl_dist_output_shape)([processed_a, processed_b])\n",
    "\n",
    "model = Model([input_a, input_b], distance)\n",
    "\n",
    "# train\n",
    "rms = RMSprop()\n",
    "model.compile(loss=contrastive_loss, optimizer=rms, metrics=[accuracy])\n",
    "model.fit([tr_pairs[:, 0], tr_pairs[:, 1]], tr_y,\n",
    "          batch_size=128,\n",
    "          epochs=epochs,\n",
    "          validation_data=([te_pairs[:, 0], te_pairs[:, 1]], te_y))\n",
    "\n",
    "# compute final accuracy on training and test sets\n",
    "y_pred = model.predict([tr_pairs[:, 0], tr_pairs[:, 1]])\n",
    "tr_acc = compute_accuracy(tr_y, y_pred)\n",
    "y_pred = model.predict([te_pairs[:, 0], te_pairs[:, 1]])\n",
    "te_acc = compute_accuracy(te_y, y_pred)\n",
    "\n",
    "print('* Accuracy on training set: %0.2f%%' % (100 * tr_acc))\n",
    "print('* Accuracy on test set: %0.2f%%' % (100 * te_acc))\n",
    "model.save('Keras_cnn_model'+str(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "colab_type": "code",
    "id": "Ey3xLMzXPSMg",
    "outputId": "1aa7d9ad-e65f-463d-f8e1-c688abf1a881"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 28, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3b2147e7b8>"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# the data, split between train and test sets\n",
    "print(images.shape)\n",
    "x_train, x_test, y_train, y_test  = train_test_split(images,labels,train_size =0.6,test_size=0.4)\n",
    "\n",
    "plt.imshow(x_train[0],cmap=\"gray\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "colab_type": "code",
    "id": "I2VR3DZZPiBs",
    "outputId": "c5a4fc36-1501-4f98-8a21-c543e7d38333"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-e7c77897d2ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_to_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/preprocessing/image.py\u001b[0m in \u001b[0;36marray_to_img\u001b[0;34m(x, data_format, scale, dtype)\u001b[0m\n\u001b[1;32m     85\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dtype'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_to_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py\u001b[0m in \u001b[0;36marray_to_img\u001b[0;34m(x, data_format, scale, dtype)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         raise ValueError('Expected image array to have rank 3 (single image). '\n\u001b[0;32m--> 257\u001b[0;31m                          'Got array with shape: %s' % (x.shape,))\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'channels_first'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'channels_last'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected image array to have rank 3 (single image). Got array with shape: (28,)"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "print(type(x_train[0]))\n",
    "display(keras.preprocessing.image.array_to_img(x_train[0][]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290
    },
    "colab_type": "code",
    "id": "jx8JF61XyW3c",
    "outputId": "f9923bdf-9b9b-4e4d-f1db-e6dcc7055693"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting eli5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/97/2f/c85c7d8f8548e460829971785347e14e45fa5c6617da374711dec8cb38cc/eli5-0.10.1-py2.py3-none-any.whl (105kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 2.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: attrs>16.0.0 in /usr/local/lib/python3.6/dist-packages (from eli5) (19.3.0)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.6/dist-packages (from eli5) (0.8.7)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from eli5) (1.18.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from eli5) (1.12.0)\n",
      "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from eli5) (0.10.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from eli5) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.6/dist-packages (from eli5) (0.22.2.post1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from eli5) (2.11.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18->eli5) (0.15.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->eli5) (1.1.1)\n",
      "Installing collected packages: eli5\n",
      "Successfully installed eli5-0.10.1\n"
     ]
    }
   ],
   "source": [
    "!pip install eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aKI7Xg-ZtPO2"
   },
   "outputs": [],
   "source": [
    "import eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HUUyjOYqxmT1"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "WS4KFgaEz-jn",
    "outputId": "a8333be4-cd35-496d-bbf2-9b8c869fd825"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (64, 1)\n"
     ]
    }
   ],
   "source": [
    "print(type(y_pred), y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "DL-Xs8JW1jJC",
    "outputId": "53ff1893-21b4-4bf7-c3e8-c03f225ea7bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (28, 28)\n"
     ]
    }
   ],
   "source": [
    "# my_pred = model.predict([te_pairs[:, 0], te_pairs[:, 1]])\n",
    "# print(type(te_pairs[:, 0]), te_pairs[:, 0].shape)\n",
    "print(type(te_pairs[0][0]), te_pairs[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9oOkkLx2183z"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "keras_siamse_network_27June.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
